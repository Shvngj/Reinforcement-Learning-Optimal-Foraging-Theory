{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -e gym-foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change parameters of the game \n",
    "\n",
    "# MaximumRows : The total length of the game is limited by time in the actual game but in the RL model it is limited by the number of row\n",
    "# MaximumAttempts : The maximum number of times a bush can be accessed before the reward collected from it becomes 0 was found to be 13 (MAX_ATTEMPTS = 13)\n",
    "# ActionTime : Time to move to the bush/square and back to the center, that is taken as ActionTime = 2 sec.\n",
    "# TimeLag : Time to move from one row to another, that is TimeLag = 3 sec for Round 1 and 10 sec for Round 2\n",
    "# env.__init__(MaximumRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_foo:bushberry-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.__init__(MaximumRows = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B represents bush and S represents square. In the game, the movement was from bottom to top, here it's from top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.ActionTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.TimeLag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before applying Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s = 20\n",
    "epochs = 0\n",
    "reward = 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "total_score = 0\n",
    "\n",
    "start = time.time()\n",
    "while time.time()-start<4*60 and not done:\n",
    "    action = env.action_space.sample()\n",
    "        \n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if action==0 or reward==10:\n",
    "        sleep(env.ActionTime/2) #time to move from center to bush or from bush to center\n",
    "    elif action==1:\n",
    "        sleep(env.TimeLag) #time to move from square to center of next row\n",
    " \n",
    "    total_score+=reward\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Before QL\n",
    "print_frames(frames)\n",
    "print('Total Score:',total_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using MVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MVT\n",
    "#Round 1: t=3 seconds\n",
    "#optimal time to spend at a bush, T = 4.37429 seconds\n",
    "import time\n",
    "\n",
    "env.s = 20\n",
    "epochs = 0\n",
    "reward = 0\n",
    "\n",
    "frames_mvt = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "total_score_mvt = 0\n",
    "\n",
    "T=4.37429\n",
    "start=time.time()\n",
    "while time.time()-start<4*60 and not done:\n",
    "    \n",
    "    start_time=time.time()\n",
    "    while True and not done:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - start_time\n",
    "        if elapsed>T:\n",
    "            action=1\n",
    "            break\n",
    "        action=0\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_score_mvt+=reward\n",
    "        frames_mvt.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        })\n",
    "        sleep(env.ActionTime/2) # time to move from center to bush or from bush to center\n",
    "            \n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    total_score_mvt+=reward\n",
    "    \n",
    "    frames_mvt.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if reward==10:\n",
    "        sleep(env.ActionTime/2) # time to move from center to bush or from bush to center\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_score_mvt+=reward\n",
    "        frames_mvt.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    sleep(env.TimeLag) # time to move from square to center of next row\n",
    "    \n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MVT\n",
    "print_frames(frames_mvt)\n",
    "print('Total Score:',total_score_mvt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "#all_penalties = []\n",
    "all_total_scores = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs,  reward = 0, 0\n",
    "    #penalties = 0\n",
    "    done = False\n",
    "    \n",
    "    total_score = 0\n",
    "    while not done:\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        total_score += reward\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performace after Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "state = env.reset()\n",
    "epochs, reward = 0, 0\n",
    "\n",
    "Qtotal_score = 0\n",
    "\n",
    "Qframes=[]\n",
    "done = False\n",
    "\n",
    "start=time.time()\n",
    "while time.time()-start<4*60 and not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if action==0 or reward==10:\n",
    "        sleep(env.ActionTime/2) #time to move from center to bush or from bush to center\n",
    "    elif action==1:\n",
    "        sleep(env.TimeLag) #time to move from square to center of next row\n",
    "\n",
    "    Qtotal_score += reward\n",
    "    \n",
    "    Qframes.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After QL\n",
    "print_frames(Qframes)\n",
    "print('Total Score:', Qtotal_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make('gym_foo:bushberry-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.__init__(MaximumRows = 50, MaximumAttempts = 13, ActionTime=2,TimeLag=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before applying Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.s = 20\n",
    "epochs = 0\n",
    "#penalties=0 \n",
    "reward = 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "total_score = 0\n",
    "start = time.time()\n",
    "while time.time()-start<4*60 and not done:\n",
    "    action = env2.action_space.sample()\n",
    "    state, reward, done, info = env2.step(action)\n",
    "    \n",
    "    if action==0 or reward==10:\n",
    "        sleep(env2.ActionTime/2) #time to move from center to bush or from bush to center\n",
    "    elif action==1:\n",
    "        sleep(env2.TimeLag) #time to move from square to center of next row\n",
    "        \n",
    "    total_score+=reward\n",
    "    #if reward == -10:\n",
    "    #    penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env2.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before QL\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(0.5)\n",
    "                       \n",
    "print_frames(frames)\n",
    "print('Total Score:',total_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MVT\n",
    "#Round 2: t=10 seconds\n",
    "#optimal time to spend at a bush, T = 7.71135 seconds\n",
    "import time\n",
    "\n",
    "env2.s = 20\n",
    "epochs = 0\n",
    "reward = 0\n",
    "\n",
    "frames_mvt = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "total_score_mvt = 0\n",
    "\n",
    "T=7.71135\n",
    "start=time.time()\n",
    "while time.time()-start<4*60 and not done:\n",
    "    \n",
    "    start_time=time.time()\n",
    "    while True and not done:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - start_time\n",
    "        if elapsed>T:\n",
    "            action=1\n",
    "            break\n",
    "        action=0\n",
    "        state, reward, done, info = env2.step(action)\n",
    "        total_score_mvt+=reward\n",
    "        frames_mvt.append({\n",
    "        'frame': env2.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        })\n",
    "        sleep(env2.ActionTime/2) # time to move from center to bush or from bush to center\n",
    "            \n",
    "    state, reward, done, info = env2.step(action)\n",
    "    \n",
    "    total_score_mvt+=reward\n",
    "    \n",
    "    frames_mvt.append({\n",
    "        'frame': env2.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if reward==10:\n",
    "        sleep(env2.ActionTime/2) # time to move from center to bush or from bush to center\n",
    "        state, reward, done, info = env2.step(action)\n",
    "        total_score_mvt+=reward\n",
    "        frames_mvt.append({\n",
    "        'frame': env2.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    sleep(env2.TimeLag) # time to move from square to center of next row\n",
    "    \n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MVT\n",
    "print_frames(frames_mvt)\n",
    "print('Total Score:',total_score_mvt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env2.observation_space.n, env2.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "#all_penalties = []\n",
    "all_total_scores = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env2.reset()\n",
    "\n",
    "    epochs,  reward = 0, 0\n",
    "    #penalties = 0\n",
    "    done = False\n",
    "    \n",
    "    total_score = 0\n",
    "    while not done:\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env2.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env2.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        #if reward == -10:\n",
    "        #    penalties += 1\n",
    "        total_score += reward\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance after Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "state = env2.reset()\n",
    "epochs, reward = 0, 0\n",
    "\n",
    "total_score = 0\n",
    "\n",
    "frames=[]\n",
    "done = False\n",
    "\n",
    "start=time.time()\n",
    "while time.time()-start<4*60 and not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env2.step(action)\n",
    "    \n",
    "    if action==0 or reward==10:\n",
    "        sleep(env2.ActionTime/2) #time to move from center to bush or from bush to center\n",
    "    elif action==1:\n",
    "        sleep(env2.TimeLag) #time to move from square to center of next row\n",
    "\n",
    "    #if reward == -10:\n",
    "    #    penalties += 1\n",
    "    total_score += reward\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env2.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After QL\n",
    "print_frames(frames)\n",
    "print('Total Score:', total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
